# Tweede Kamer Open Data Pipeline

Data pipeline for fetching and preprocessing **all** parliamentary data from the [Dutch House of Representatives (Tweede Kamer)](https://opendata.tweedekamer.nl/) OData API.

## What's in the data?

35+ entity types covering the full Dutch parliamentary process:

| Category | Entities |
|---|---|
| **People** | Persoon, PersoonContactinformatie, PersoonGeschenk, PersoonLoopbaan, PersoonNevenfunctie, PersoonNevenfunctieInkomsten, PersoonOnderwijs, PersoonReis |
| **Parties (Fracties)** | Fractie, FractieAanvullendGegeven, FractieZetel, FractieZetelPersoon, FractieZetelVacature |
| **Committees** | Commissie, CommissieContactinformatie, CommissieZetel, CommissieZetelVast/VervangerPersoon/Vacature |
| **Activities** | Activiteit, ActiviteitActor, Agendapunt, Reservering |
| **Documents** | Document, DocumentActor, DocumentVersie, DocumentPublicatie, DocumentPublicatieMetadata, Kamerstukdossier |
| **Decisions & Votes** | Besluit, Stemming |
| **Cases** | Zaak, ZaakActor |
| **Meetings** | Vergadering, Verslag, Toezegging |
| **Rooms** | Zaal |

---

## Team Workflow — Getting the Same Data

> **Data is never committed to git.** Everyone runs the pipeline to produce identical data locally.
> This keeps the repo small and ensures everyone always works with the latest data from the API.

### First time setup

```bash
# 1. Clone the repo
git clone <repo-url> && cd <repo-name>

# 2. Setup environment + install dependencies
make setup

# 3. Fetch & process all data (takes ~10-20 min depending on API)
make data
```

### Day-to-day

```bash
# Pull latest code, then regenerate data so everyone is in sync:
git pull
make data
```

### When you change the pipeline

If you modify `config.yaml`, `src/fetch.py`, or `src/preprocess.py`:

1. **Test it** — run `make data` and verify output with `make summary`
2. **Commit your code changes** — the data itself is gitignored
3. **Tell the team** — after they pull, they run `make data` and get the same result

### Quick reference

| Command | What it does |
|---|---|
| `make setup` | Create venv + install deps (first time only) |
| `make data` | Full pipeline: fetch + preprocess all entities |
| `make fetch` | Fetch raw JSON only (skip preprocessing) |
| `make process` | Preprocess only (raw data must already exist) |
| `make summary` | Print summary table of all processed data |
| `make list` | List all available entity types |
| `make viz` | Generate data overview dashboard (dashboard.png) |
| `make clean` | Delete all data files (keep code) |
| `make nuke` | Delete data + venv (full reset) |

### Without Make

If you don't have `make` (Windows without WSL), use the Python commands directly:

```bash
# Setup
python3 -m venv venv
source venv/bin/activate        # Linux/Mac
# venv\Scripts\activate         # Windows
pip install -r requirements.txt

# Full pipeline
python pipeline.py

# Other options
python pipeline.py --fetch-only
python pipeline.py --preprocess-only
python pipeline.py --entities Persoon Fractie Stemming
python pipeline.py --summary
python pipeline.py --list-entities
python pipeline.py -v               # verbose logging
```

---

## Project Structure

```
.
├── Makefile             # One-command team workflow (make data, make clean, etc.)
├── pipeline.py          # Main CLI entry point
├── visualize.py         # Generate data overview dashboard
├── config.yaml          # Pipeline configuration (entities, API settings, etc.)
├── requirements.txt     # Python dependencies
├── src/
│   ├── fetch.py         # OData API fetcher with pagination & retries
│   └── preprocess.py    # Data cleaning & normalization
├── data/
│   ├── raw/             # Raw JSON from API (git-ignored)
│   │   └── .gitkeep
│   └── processed/       # Clean Parquet/CSV files (git-ignored)
│       └── .gitkeep
├── .gitignore           # Smart ignore: all data files excluded, code tracked
└── README.md
```

## What's gitignored and why

| Ignored | Reason |
|---|---|
| `data/raw/`, `data/processed/` | Generated by the pipeline — everyone fetches their own |
| `*.parquet`, `*.csv`, `*.json` | Data files, even if placed outside `data/` by accident |
| `*.pkl`, `*.sqlite`, `*.duckdb` | Common analysis artifacts |
| `*.xlsx`, `*.xls` | Spreadsheet exports |
| `venv/`, `.venv/` | Virtual environments are local |
| `__pycache__/`, `*.pyc` | Python bytecode |
| `.env`, `secrets.yaml` | Credentials — never commit these |

**Exceptions** (tracked despite matching patterns above):
- `config.yaml` — pipeline configuration, must be in sync across the team
- `data/raw/.gitkeep`, `data/processed/.gitkeep` — preserves directory structure

## Configuration

Edit `config.yaml` to customize:

- **API settings**: rate limiting, retries, timeouts
- **Entities**: enable/disable specific entity types
- **Preprocessing**: date parsing, null column removal, output format (parquet/csv)

## How it works

1. **Fetch**: For each enabled entity, the pipeline queries the OData v4 API at `https://gegevensmagazijn.tweedekamer.nl/OData/v4/2.0/{Entity}` with `$filter=Verwijderd eq false` to exclude deleted records. It handles pagination (250 records/page) automatically via `@odata.nextLink`.

2. **Preprocess**: Raw JSON is loaded into pandas DataFrames, then:
   - Redundant `Verwijderd` column is dropped
   - UUID IDs are standardized to lowercase
   - String columns are normalized (whitespace stripped)
   - Datetime columns are auto-detected and parsed
   - All-null columns are removed
   - Output is saved as Parquet (or CSV)

3. **Summary**: A `_summary.csv` is generated with record counts, column counts, and file sizes for every entity.

## API Reference

- Portal: https://opendata.tweedekamer.nl/
- OData API docs: https://opendata.tweedekamer.nl/documentatie/odata-api
- Information model: https://opendata.tweedekamer.nl/documentatie/informatiemodel
- Base URL: `https://gegevensmagazijn.tweedekamer.nl/OData/v4/2.0`
